# ğŸ§  Sentiment Analysis Microservice

An end-to-end containerized microservice for **binary sentiment classification** using Hugging Face Transformers. Includes:

- Python **FastAPI backend** for inference
- Fine-tuning script with CLI
- Minimal **React frontend** (Vite) for user input
- **Docker Compose** setup for full local development

---

## ğŸš€ Setup & Run Instructions

### ğŸ”§ Prerequisites

- Docker & Docker Compose installed
- (Optional) Python 3.10+ and Node.js for local development without Docker

---

### ğŸ³ Run Everything via Docker

From the project root:

```bash
docker-compose up --build
```

- Backend (FastAPI) â†’ http://localhost:8080
- Frontend (React) â†’ http://localhost:3000

### ğŸ§ª Test the API

Swagger UI available at:
```
http://localhost:8080/docs
```

### ğŸ›  Design Decisions

* FastAPI backend with Hugging Face pipeline() abstraction for simplicity and speed
* Automatically loads latest fine-tuned model from ```./model``` on container start
* Fine-tune script ```(finetune.py)``` saves weights directly to ```./model/```
* React frontend uses fetch to call backend /predict API
* Fully containerized with Docker Compose, using CPU-only compatible base images
* Supports model formats with ```pytorch_model.bin```   

### ğŸ§ª Fine-Tuning

Use the provided CLI:
```bash
python finetune.py -data data.jsonl -epochs 3 -lr 3e-5
```

Each line of data.jsonl should look like:
```json
{"text": "Amazing service!", "label": "positive"}
```
Fine-tuned weights are saved to ./model/ and loaded by the backend automatically.


### â±ï¸ CPU Fine-Tune Times

| Hardware     | Dataset Size | Epochs | Time        |
| ------------ | ------------ | ------ | ----------- |
| CPU (8-core) | 50 samples   | 10      | \~30â€“40 sec |

â± Fine-tune time will scale with dataset size. For <1000 samples, CPU is sufficient

### ğŸ”Œ API Reference

```POST /predict```

**Request:**
```json
{
  "text": "I love this!"
}
```
**Response:**
```json
{
  "label": "positive",
  "score": 0.9823
}
```